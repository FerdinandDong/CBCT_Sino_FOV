# configs/train_i2sb_local_multi.yaml
# Train config aligned with your existing cfg style (same field system as sample_i2sb_local_multi.yaml)
# For: scripts/i2sb_train.py + ctprojfix/trainers/i2sb_local_multi.py (I2SBLocalTrainer)

model:
  name: i2sb_local_multi
  params:
    # xt(1) + x1_img(1) + conds(mask+angle=2) + t_map(1) = 5
    # if add_angle_channel=false then in_ch should be 4
    in_ch: 5
    base: 64
    depth: 4
    emb_dim: 256
    dropout: 0.0

data:
  use_dummy: false

  # dataset roots (keep consistent with your sample cfg)
  root_noisy: /data/shared_folder/projectionsNoisy
  root_clean: /data/shared_folder/projectionsNoisy

  # optional filters
  ids: []
  # exclude_ids: [0, 10, 11]
  exclude_ids: []

  # projection sampling
  step: 1
  downsample: 2           # 用于快速验证；最终可改回 2
  truncate_left: 350
  truncate_right: 350

  mask_mode: fixed
  normalize: percentile
  add_angle_channel: true

  # dataloader
  batch_size: 8       # ds8 32    ds2 8
  num_workers: 8
  pin_memory: true
  shuffle: false
  return_norm_stats: true
  drop_last: true

  # split definition (same as sample cfg)
  split:
    mode: files
    list_format: id
    train_list: splits/debug_train.txt
    val_list:   splits/debug_val.txt
    # test_list:  splits/test.txt
    # train_list: splits/train.txt
    # val_list:   splits/val.txt
    test_list:  splits/test.txt
    temp_test_list: splits/temp_test.txt

train:
  # ---------------- basic ----------------
  device: "cuda:1" # 前台2 后台1
  lr: 2.0e-4
  epochs: 150     # 和续训相关

  # ---------------- I2SB bridge ----------------
  sigma_T: 1.0
  t0: 1.0e-4

  # ---------------- EMA ----------------
  ema_decay: 0.999

  # ---------------- ckpt/log ----------------
  ckpt_dir: checkpoints/i2sb_local_multi
  ckpt_prefix: i2sb_local_multi
  log_dir: logs/i2sb_local_multi
  # ckpt_dir: checkpoints/i2sb_local_multi_ds8
  # ckpt_prefix: i2sb_local_multi_ds8
  # log_dir: logs/i2sb_local_multi_ds8
  # ckpt_dir: checkpoints/i2sb_local_multi_ds4
  # ckpt_prefix: i2sb_local_multi_ds4
  # log_dir: logs/i2sb_local_multi_ds4

  save_every: 1
  max_keep: 5
  log_interval: 100

  # 预览图：由 trainer 的 _dump_preview 控制
  # dump_preview_every=0 关闭；=1 每个epoch出一次 preview_epochXXX.png
  dump_preview_every: 30

  # ---------------- conditioning ----------------
  # 保持跟 data.add_angle_channel 一致
  # scripts/i2sb_train.py 目前是从 data.add_angle_channel  cond_has_angle
  # cond_has_angle: true

  # ---------------- val ----------------
  eval_every: 10
  val_max_batches: 8

  # best selection metric
  # "loss" / "psnr" / "ssim"
  val_metric: psnr
  maximize_metric: true

  # ---------------- scheduler ----------------
  # null / cosine / step
  sched:
    type: cosine
    T_max: 150
    eta_min: 1.0e-6
    step_size: 50
    gamma: 0.5

  # ---------------- outpainting weights ----------------
  w_valid: 1.0
  w_missing: 10.0

  # ---------------- perceptual loss ----------------
  # 需要 torchvision；不开就不会引入额外依赖
  use_percep: true
  w_percep: 0.06    # 0.05
  percep_layers: [3, 8]
  percep_use_pretrained: true
  # "missing" | "valid" | "both"
  percep_region: both
  percep_max_hw: 256 # 256 384对于ds4 1024?

  # ---------------- resume ----------------
  # 当前 trainer 的 auto 逻辑是找 prefix_epoch*.pth（不包含 *_last.pth / *_best.pth）
  # 写死 last 路径是最稳的方式
  
  #从头开始:
  # epochs: 150
  # resume: none
  # resume_from: null
  # strict_load: true

  #真续训：从上次中断的 last/best 自动继续
  # epochs: 150
  # resume: auto
  # resume_from: null
  # resume_from: checkpoints/i2sb_local_multi/i2sb_local_multi_last.pth
  # resume: last
  # strict_load: true  
# train:
#   epochs: 300
#   sched:
#     type: cosine
#     T_max: 300
  
  # 伪续训/迁移 warm start：从指定 ckpt 继续训练
  epochs: 150
  init_from: checkpoints/i2sb_local_multi/i2sb_local_multi_last.pth # 只要 init_from 生效，脚本会自动 resume=none 且 从 start_epoch 开始循环
  init_strict: true # 是否严格加载（默认 true）
  start_epoch: 0
 
  # # 选项: 伪续训要避免覆盖原实验
  ckpt_prefix: i2sb_local_multi_ft1
  ckpt_dir: checkpoints/i2sb_local_multi_ft1
  log_dir: logs/i2sb_local_multi_ft1

  # ---------------- inference / sampling for val/preview ----------------
  # 这些参数需要把它们加进 trainer 的 __init__ 并在 _eval_val/_dump_preview 中使用

  val_infer: sample            # "one_step" | "sample"
  sample_steps: 5
  sample_stochastic: false     # 走 ODE 还是走 SDE!!!!!!!!!!!!!!!!!!!!!!!!!!
  sample_clamp_known: false    # 强制已知区域不变
